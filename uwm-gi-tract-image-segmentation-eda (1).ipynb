{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<br><center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\" width=100%></center>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">UWM - GI Tract Image Segmentation Challenge - EDA</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: JYOTI DABASS</h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üõë &nbsp; WARNING:</b><br><br><b>THIS IS A WORK IN PROGRESS</b><br>\n</div></center>\n\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n</div></center>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#create_dataset\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#modelling\">5&nbsp;&nbsp;&nbsp;&nbsp;MODELLING</a></h3>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: teal;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_hub as tfhub; print(f\"\\t\\t‚Äì TENSORFLOW HUB VERSION: {tfhub.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom scipy.spatial import cKDTree\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:02:40.054268Z","iopub.execute_input":"2022-08-17T07:02:40.054856Z","iopub.status.idle":"2022-08-17T07:02:50.597205Z","shell.execute_reply.started":"2022-08-17T07:02:40.054727Z","shell.execute_reply":"2022-08-17T07:02:50.596244Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION</b>\n\nIn this competition, you‚Äôll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">BASIC BACKGROUND INFORMATION</b>\n\nIn 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, <b>oncologists are able to visualize the daily position of the tumor and intestines, <mark>which can vary day to day</mark></b>. \n\nIn these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate‚Äîunless deep learning could help automate the segmentation process. <b><mark>A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.</mark></b>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION HOST INFORMATION</b>\n\nThe UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">VISUAL EXPLANATION</b>\n\n<center><img src=\"https://lh5.googleusercontent.com/zbBUgbj1jyZxyu3r1vr5zKKr8yK1hSdwAM3HpD_n6j2W-5-wKP3ZRusi_3yskSgnC-tMRKqOEtLycbLkTWCJAUe4Cylv_VsW81DYI4ray02uZLeSnlzAuZRIU7L2Q0KURYSMqFI\"></center><br>\n\n<i>The tumor above (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. Dose levels are represented by colour. Higher doses are represented by red and lower doses are represented by green.</i><br>\n\n<br><center><img src=\"https://www.humonc.wisc.edu/wp-content/uploads/2017/09/Bayouth_Project4_72ppi.png\"></center><br>\n\n<i>MRI is an excellent imaging modality for visualization of soft tissues. This is particularly useful for tumors of the abdomen, such as pancreatic cancer shown below.  The left image shows the patient‚Äôs anatomy during exhale, while the image on the right shows the anatomical change during a maximum inspiration breath hold (MIBH). In the MIBH image we can see motion of nearly all the soft tissue, providing us superior ability to align the tumor during our treatment delivery. We are analyzing the clinical impact of using these treatment planning and delivery techniques and our patient‚Äôs ability to comply with self-guided breathing maneuvers.<b><a href=\"https://www.humonc.wisc.edu/research/medical-physics_research/mr-guided-radiation-therapy-research-2/\">[REF]</a></b></i>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION IMPACT STATEMENT</b>\n\nCancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.2 COMPETITION EVALUATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n\nThis competition is evaluated on the mean <a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\"><b>Dice coefficient</b></a> and <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>3D Hausdorff distance</b></a>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$\n\\frac{2 * |X \\cap Y|}{|X| + |Y|}\n$$\n\nwhere $X$ is the predicted set of pixels and $Y$ is the ground truth. \n* The Dice coefficient is defined to be $1$ when both $X$ and $Y$ are empty. \n* The leaderboard score is the <b>mean of the Dice coefficients for each image in the test set.</b>\n\nHausdorff distance is a method for calculating the distance between segmentation objects A and B, by calculating the furthest point on object A from the nearest point on object B. For 3D Hausdorff, we construct 3D volumes by combining each 2D segmentation with slice depth as the Z coordinate and then find the Hausdorff distance between them. **(In this competition, the slice depth for all scans is set to 1.)** <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>The scipy code for Hausdorff is linked</b></a>. The expected / predicted pixel locations are normalized by image size to create a bounded 0-1 score.\n\n<br>\n    \n---\n\n<b>NOTE: The two metrics are combined during evaluation!</b>\n\n* <b>Weight of 0.4 for the Dice metric</b>\n* <b>Weight of 0.6 for the Hausdorff distance.</b>\n\n---\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FILE INFORMATION</b>\n\nIn order to reduce the submission file size, our metric uses **run-length encoding** on the pixel values.  \n* Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length\n* E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n* Note that, at the time of encoding, the mask should be **binary**\n    * The masks for all objects in an image are joined into a single large mask\n    * The value of **0** should indicate pixels that are not **masked**\n    * The value of **1** will indicate pixels that are **masked**.\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\n<br>\n\nThe file should contain a header and have the following format:\n\n```\nid,class,predicted\n1,large_bowel,1 1 5 1\n1,small_bowel,1 1\n1,stomach,1 1\n2,large_bowel,1 5 2 17\netc.\n```\n\n<br><font color=\"red\"><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">IS THIS A CODE COMPETITION?</b></font>\n\n<font color=\"red\" style=\"font-size: 30px\"><b>YES</b></font>\n\n<br>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.3 DATASET OVERVIEW</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n\n<b><mark>In this competition we are segmenting organs cells in images</mark></b>. \n\nThe training **<mark>annotations are provided as RLE-encoded masks</mark>**, and the images are in **<mark>16-bit</mark>**, **<mark>grayscale</mark>**, **<mark>PNG format</mark>**.\n\nEach case in this competition is represented by multiple sets of scan slices\n* Each set is identified by the day the scan took place\n* Some cases are split by time\n    * early days are in train\n    * later days are in test\n* Some cases are split by case\n    * the entirety of the case is in train or test\n\n<b><mark>The goal of this competition is to be able to generalize to both partially and wholly unseen cases.</mark></b>\n\nNote that, in this case, the test set is entirely unseen.\n* It is roughly 50 cases\n* It contains a varying number of days and slices, (similar to the training set)\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">FILE INFORMATION</b>\n\n**`train.csv`** \n- IDs and masks for all training objects.\n- **Columns**\n    * **`id`**\n        * unique identifier for object\n    * **`class`**\n        * the predicted class for the object\n    * **`EncodedPixels`**\n        * RLE-encoded pixels for the identified object\n\n<br>\n\n**`sample_submission.csv`**\n- A sample submission file in the correct format\n\n<br>\n\n**`train/`**\n- a folder of case/day folders, each containing slice images for a particular case on a given day.\n\n<br>\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">‚ö†Ô∏è &nbsp; NOTE &nbsp; ‚ö†Ô∏è</b><br><br><b style=\"font-size: 22px; color: darkorange\"></b><br><br>The <b>image filenames</b> include 4 numbers <b>(ex. 276_276_1.63_1.63.png)</b>.<br><br>These four numbers are representative of:<ul><li><b>slice height</b> (integer in pixels)</li><li><b>slice width</b> (integer in pixels)</li><li><b>heigh pixel spacing</b> (floating point in mm)</li><li><b>width pixel spacing</b> (floating point in mm)</li></ul><br>The first two defines the resolution of the slide. The last two record the physical size of each pixel.<br><br>\n</div></center>\n\n \n\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n\n---\n\nIn order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n\n1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n\n<br>\n\n```python\n# The name you gave to the TPU to use\nTPU_WORKER = 'my-tpu-name'\n\n# or you can also specify the grpc path directly\n# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n\n# The zone you chose when you created the TPU to use on GCP.\nZONE = 'us-east1-b'\n\n# The name of the GCP project where you created the TPU to use on GCP.\nPROJECT = 'my-tpu-project'\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCES:</b><br><br>\n    - <a href=\"https://www.tensorflow.org/guide/tpu#tpu_initialization\"><b>Guide - Use TPUs</b></a><br>\n    - <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\"><b>Doc - TPUClusterResolver</b></a><br>\n\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:02:50.599144Z","iopub.execute_input":"2022-08-17T07:02:50.599987Z","iopub.status.idle":"2022-08-17T07:02:50.619975Z","shell.execute_reply.started":"2022-08-17T07:02:50.599937Z","shell.execute_reply":"2022-08-17T07:02:50.619057Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('uw-madison-gi-tract-image-segmentation')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:02:50.621632Z","iopub.execute_input":"2022-08-17T07:02:50.622193Z","iopub.status.idle":"2022-08-17T07:02:50.643398Z","shell.execute_reply.started":"2022-08-17T07:02:50.622116Z","shell.execute_reply":"2022-08-17T07:02:50.642303Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n\n---\n\n\n**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n\n<br>\n\nWhen a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n\nXLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; NOTE:</b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCE:</b><br><br>    - <a href=\"https://www.tensorflow.org/xla\"><b>XLA: Optimizing Compiler for Machine Learning</b></a><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:02:50.645702Z","iopub.execute_input":"2022-08-17T07:02:50.646232Z","iopub.status.idle":"2022-08-17T07:02:50.654021Z","shell.execute_reply.started":"2022-08-17T07:02:50.646184Z","shell.execute_reply":"2022-08-17T07:02:50.652700Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\n# Open the training dataframe and display the initial dataframe\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# Get all training images\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\nTEST_DIR = os.path.join(DATA_DIR, \"test\")\nSS_CSV   = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\n\n# Get all testing images if there are any\nall_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\n# For debugging purposes when the test set hasn't been substituted we will know\nDEBUG=len(ss_df)==0\n\nif DEBUG:\n    TEST_DIR = TRAIN_DIR\n    all_test_images = all_train_images\n    ss_df = train_df.iloc[:10]\n    ss_df = ss_df[[\"id\", \"class\"]]\n    ss_df[\"predicted\"] = \"\"\n    \n    print(\"\\n\\n\\n... DEBUG SUBMISSION DATAFRAME... \\n\")\n    display(ss_df)\n\n    \n\nSF2LF = {\"lb\":\"Large Bowel\",\"sb\":\"Small Bowel\",\"st\":\"Stomach\"}\nLF2SF = {v:k for k,v in SF2LF.items()}\nprint(f\"\\n\\n\\n... ARE WE DEBUGGING: {DEBUG}... \\n\")\n\nprint(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:02:50.655576Z","iopub.execute_input":"2022-08-17T07:02:50.656312Z","iopub.status.idle":"2022-08-17T07:03:03.320774Z","shell.execute_reply.started":"2022-08-17T07:02:50.656266Z","shell.execute_reply":"2022-08-17T07:03:03.319914Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.5 UPDATE DATAFRAMES WITH ACCESSIBLE EXTRA INFORMATION</h3>\n\n---\n\nI wrapped the logic in a preprocessing function but also went through step by step so people could validate if they so wished\n\n**NOTE: I have changed the column identifiers as follows for the sake of brevity:**\n* **large_bowel** --> **lb**\n* **small_bowel** --> **sb**\n* **stomach** --> **st**","metadata":{}},{"cell_type":"code","source":"def get_filepath_from_partial_identifier(_ident, file_list):\n    return [x for x in file_list if _ident in x][0]\n\ndef df_preprocessing(df, globbed_file_list, is_test=False):\n    \"\"\" The preprocessing steps applied to get column information \"\"\"\n    # 1. Get Case-ID as a column (str and int)\n    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n\n    # 2. Get Day as a column\n    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n\n    # 3. Get Slice Identifier as a column\n    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n\n    # 4. Get full file paths for the representative scans\n    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n                           \"/scans/\"+df[\"slice_id\"]) # .../slice_#### \n    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n\n    # 5. Get slice dimensions from filepath (int in pixels)\n    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n\n    # 6. Pixel spacing from filepath (float in mm)\n    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n\n    if not is_test:\n        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int)+df[\"sb_seg_flag\"].astype(int)+df[\"st_seg_flag\"].astype(int)\n\n    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n                     \"lb_seg_rle\", \"lb_seg_flag\",\n                     \"sb_seg_rle\", \"sb_seg_flag\", \n                     \"st_seg_rle\", \"st_seg_flag\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n                     \"day_num_str\", \"day_num\", \"slice_id\",]\n    if is_test: new_col_order.insert(1, \"class\")\n    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n    df = df[new_col_order]\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:03.322827Z","iopub.execute_input":"2022-08-17T07:03:03.323387Z","iopub.status.idle":"2022-08-17T07:03:03.542493Z","shell.execute_reply.started":"2022-08-17T07:03:03.323340Z","shell.execute_reply":"2022-08-17T07:03:03.541224Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION STARTED ...\\n\\n\")\n\n# 1. Get Case-ID as a column (str and int)\ntrain_df[\"case_id_str\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\ntrain_df[\"case_id\"] = train_df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n\n# 2. Get Day as a column\ntrain_df[\"day_num_str\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\ntrain_df[\"day_num\"] = train_df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n\n# 3. Get Slice Identifier as a column\ntrain_df[\"slice_id\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n\n# 4. Get full file paths for the representative scans\ntrain_df[\"_partial_ident\"] = (TRAIN_DIR+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                             train_df[\"case_id_str\"]+\"/\"+ # .../case###/\n                             train_df[\"case_id_str\"]+\"_\"+train_df[\"day_num_str\"]+ # .../case###_day##/\n                             \"/scans/\"+train_df[\"slice_id\"]) # .../slice_#### \n_tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in all_train_images], \"f_path\":all_train_images})\ntrain_df = train_df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n\n# Minor cleanup of our temporary workaround\ndel _tmp_merge_df; gc.collect(); gc.collect()\n\n# 5. Get slice dimensions from filepath (int in pixels)\ntrain_df[\"slice_h\"] = train_df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\ntrain_df[\"slice_w\"] = train_df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n\n# 6. Pixel spacing from filepath (float in mm)\ntrain_df[\"px_spacing_h\"] = train_df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\ntrain_df[\"px_spacing_w\"] = train_df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n\n# 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\nl_bowel_train_df = train_df[train_df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\ns_bowel_train_df = train_df[train_df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\nstomach_train_df = train_df[train_df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\ntrain_df = train_df.merge(l_bowel_train_df, on=\"id\", how=\"left\")\ntrain_df = train_df.merge(s_bowel_train_df, on=\"id\", how=\"left\")\ntrain_df = train_df.merge(stomach_train_df, on=\"id\", how=\"left\")\ntrain_df = train_df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\ntrain_df[\"lb_seg_flag\"] = train_df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\ntrain_df[\"sb_seg_flag\"] = train_df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\ntrain_df[\"st_seg_flag\"] = train_df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\ntrain_df[\"n_segs\"] = train_df[\"lb_seg_flag\"].astype(int)+train_df[\"sb_seg_flag\"].astype(int)+train_df[\"st_seg_flag\"].astype(int)\n\n# 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\ntrain_df = train_df[[\"id\", \"f_path\", \"n_segs\",\n                     \"lb_seg_rle\", \"lb_seg_flag\",\n                     \"sb_seg_rle\", \"sb_seg_flag\", \n                     \"st_seg_rle\", \"st_seg_flag\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n                     \"day_num_str\", \"day_num\", \"slice_id\",]]\n\n# 9. Display update dataframe\nprint(\"\\n... UPDATED TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\nss_df = df_preprocessing(ss_df, all_test_images, is_test=True)\nprint(\"\\n\\n\\n... UPDATED SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\nprint(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:03.544692Z","iopub.execute_input":"2022-08-17T07:03:03.545011Z","iopub.status.idle":"2022-08-17T07:03:05.747937Z","shell.execute_reply.started":"2022-08-17T07:03:03.544971Z","shell.execute_reply":"2022-08-17T07:03:05.747063Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)\n\n# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\ndef rle_decode_top_to_bot_first(mask_rle, shape):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns:\n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    \n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef load_json_to_dict(json_path):\n    \"\"\" tbd \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\ndef tf_load_png(img_path):\n    return tf.image.decode_png(tf.io.read_file(img_path), channels=3)\n\ndef open_gray16(_path, normalize=True, to_rgb=False):\n    \"\"\" Helper to open files \"\"\"\n    if normalize:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535., axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535.\n    else:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH), axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:05.749693Z","iopub.execute_input":"2022-08-17T07:03:05.750195Z","iopub.status.idle":"2022-08-17T07:03:05.769436Z","shell.execute_reply.started":"2022-08-17T07:03:05.750132Z","shell.execute_reply":"2022-08-17T07:03:05.768449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"create_dataset\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"create_dataset\">\n    4&nbsp;&nbsp;DATASET EXPLORATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.0 LOOK AT A SINGLE EXAMPLE PRIOR TO INVESTIGATION</h3>\n\n---\n\nWe simply do this to make sure everything is where it should be and we understand the basics of how to access all the relevant data.\n\nWe will wrap this basic exploration functionality as single function to allow for easy examination of any passed identifier","metadata":{}},{"cell_type":"code","source":"def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    _img = open_gray16(img_path, to_rgb=True)\n    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) if rle_str is not None else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay\n\ndef examine_id(ex_id, df=train_df, plot_overlay=True, print_meta=False, plot_grayscale=False, plot_binary_segmentation=False):\n    \"\"\" Wrapper function to allow for easy visual exploration of an example \"\"\"\n    print(f\"\\n... ID ({ex_id}) EXPLORATION STARTED ...\\n\\n\")\n    demo_ex = df[df.id==ex_id].squeeze()\n\n    if print_meta:\n        print(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\n        display(demo_ex.to_frame())\n\n    if plot_grayscale:\n        print(f\"\\n\\n... GRAYSCALE IMAGE PLOT ...\\n\")\n        plt.figure(figsize=(12,12))\n        plt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\n        plt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.show()\n\n    if plot_binary_segmentation:\n        print(f\"\\n\\n... BINARY SEGMENTATION MASKS ...\\n\")\n        plt.figure(figsize=(20,10))\n        for i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n            if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n            plt.subplot(1,3,i+1)\n            plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n            plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n            plt.axis(False)\n        plt.tight_layout()\n        plt.show()\n\n    if plot_overlay:\n        print(f\"\\n\\n... IMAGE WITH RGB SEGMENTATION MASK OVERLAY ...\\n\")\n        # We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n        # _img = open_gray16(demo_ex.f_path, to_rgb=True)\n        #_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n        #_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\n        #seg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n                                      #src2=_seg_rgb, beta=0.33, gamma=0)\n        _rle_strs = [demo_ex[f\"{_seg_type}_seg_rle\"] if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else None for _seg_type in [\"lb\", \"sb\", \"st\"]]\n        seg_overlay = get_overlay(demo_ex.f_path, _rle_strs, img_shape=(demo_ex.slice_w, demo_ex.slice_h))\n\n        plt.figure(figsize=(12,12))\n        plt.imshow(seg_overlay)\n        plt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n        plt.legend(handles,labels)\n        plt.axis(False)\n        plt.show()\n\n    print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:05.770676Z","iopub.execute_input":"2022-08-17T07:03:05.770899Z","iopub.status.idle":"2022-08-17T07:03:05.790138Z","shell.execute_reply.started":"2022-08-17T07:03:05.770871Z","shell.execute_reply":"2022-08-17T07:03:05.789319Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... SINGLE ID EXPLORATION STARTED ...\\n\\n\")\n\nDEMO_ID = \"case123_day20_slice_0082\"\ndemo_ex = train_df[train_df.id==DEMO_ID].squeeze()\n\nprint(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\ndisplay(demo_ex.to_frame())\n\nprint(f\"\\n\\n... LET'S PLOT THE IMAGE FIRST ...\\n\")\nplt.figure(figsize=(12,12))\nplt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\nplt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\nplt.axis(False)\nplt.show()\n\nprint(f\"\\n\\n... LET'S PLOT THE 3 SEGMENTATION MASKS ...\\n\")\n\nplt.figure(figsize=(20,10))\nfor i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n    if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n    plt.subplot(1,3,i+1)\n    plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n    plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\\n... LET'S PLOT THE IMAGE WITH AN RGB SEGMENTATION MASK OVERLAY ...\\n\")\n\n# We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n_img = open_gray16(demo_ex.f_path, to_rgb=True)\n_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\nseg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n                              src2=_seg_rgb, beta=0.33, gamma=0.0)\n\nplt.figure(figsize=(12,12))\nplt.imshow(seg_overlay)\nplt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\nhandles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\nlabels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\nplt.legend(handles,labels)\nplt.axis(False)\nplt.show()\n\nprint(f\"\\n\\n... LET'S PRINT THE RELEVANT INFORMATION ...\\n\")\nprint(f\"\\t--> IMAGE CASE ID              : {demo_ex.case_id}\")\nprint(f\"\\t--> IMAGE DAY NUMBER           : {demo_ex.day_num}\")\nprint(f\"\\t--> IMAGE SLICE WIDTH          : {demo_ex.slice_w}\")\nprint(f\"\\t--> IMAGE SLICE HEIGHT         : {demo_ex.slice_h}\")\nprint(f\"\\t--> IMAGE PIXEL SPACING WIDTH  : {demo_ex.px_spacing_w}\")\nprint(f\"\\t--> IMAGE PIXEL SPACING HEIGHT : {demo_ex.px_spacing_h}\")\n\nprint(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:05.793143Z","iopub.execute_input":"2022-08-17T07:03:05.793425Z","iopub.status.idle":"2022-08-17T07:03:06.899990Z","shell.execute_reply.started":"2022-08-17T07:03:05.793394Z","shell.execute_reply":"2022-08-17T07:03:06.899106Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Since we made the function... let's iterate over some examples where there is a tumor present in all locales and plot them**","metadata":{}},{"cell_type":"code","source":"# Plot 10 random-ids where all tumor locales are present (max one id per case)\nN_TO_PLOT = 10\nfor _id in train_df[train_df.n_segs==3].groupby(\"case_id\")[\"id\"].first().sample(N_TO_PLOT):\n    examine_id(_id)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:06.901422Z","iopub.execute_input":"2022-08-17T07:03:06.901664Z","iopub.status.idle":"2022-08-17T07:03:10.394303Z","shell.execute_reply.started":"2022-08-17T07:03:06.901633Z","shell.execute_reply":"2022-08-17T07:03:10.393369Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.1 INVESTIGATE THE OCCURENCE SEGMENTATION MAP TYPES</h3>\n\n---\n\nIt's quite apparent that not all images have segmentation maps for the various regions (stomach, large-bowel, small-bowel), so we will identify the frequency for which these occur independently... as well as the frequency for which these maps co-occur.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* There are **38,496** total examples.\n* It can be observed that more than half of the given examples have no annotations present!\n    * There are **21,906** (56.9046%) examples with no annotations/masks/segmentation present\n    * Inversely there are **16,590** (43.0954%) examples with one or more annotations present\n* There are **2,468** (6.41%) examples with **one annotation present**. \n* It can be observed that the vast majority of single mask annotations are **Stomach**!\n    * Of these annotations, **2286** (~92.6%) are **Stomach**\n    * Of these annotations, **123** (~4.98%) are **Large Bowel**\n    * Of these annotations, **59** (~2.39%) are **Small Bowel**\n* There are **10,921** (28.37%) examples with **two annotations present**. \n* It can be observed, in contrast to the single annotation examples, that the majority of annotations do NOT include stomach i.e. **'Large Bowel, Small Bowel'**!\n    * Of these annotations, **7781** (~71.3%) are **'Large Bowel, Small Bowel'**\n    * Of these annotations, **2980** (~27.3%) are **'Large Bowel, Stomach'**\n    * Of these annotations, **160** (~1.47%) are **'Small Bowel, Stomach'**\n* Finally, there are **3,201** (8.32%) examples with **all three annotations present**. \n\n<!--  # print(len(train_df))\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"No Mask\"]), len(train_df[train_df[\"seg_combo_str\"]==\"No Mask\"])/len(train_df))\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel\"])/2468)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Small Bowel\"])/2468)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Stomach\"])/2468)\n# print(len(train_df[train_df[\"seg_combo_str\"].apply(lambda x: x.count(\",\")==1)]))\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel, Stomach\"])/10921)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel, Small Bowel\"])/10921)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Small Bowel, Stomach\"])/10921)\n# print(len(train_df[train_df[\"seg_combo_str\"].apply(lambda x: x.count(\",\")==2)])) -->","metadata":{}},{"cell_type":"code","source":"def get_seg_combo_str(row):\n    seg_str_list = []\n    if row[\"lb_seg_flag\"]: seg_str_list.append(\"Large Bowel\")\n    if row[\"sb_seg_flag\"]: seg_str_list.append(\"Small Bowel\")\n    if row[\"st_seg_flag\"]: seg_str_list.append(\"Stomach\")\n    if len(seg_str_list)>0:\n        return \", \".join(seg_str_list)\n    else:\n        return \"No Mask\"\ntrain_df[\"seg_combo_str\"] = train_df.progress_apply(get_seg_combo_str, axis=1)\n\nfig = px.histogram(train_df, train_df[\"n_segs\"].astype(str), color=\"seg_combo_str\", title=\"<b>Number of Segmentation Masks Per Image</b>\", \n                  labels={\"x\":\"Number of Segmentation Masks Per Image\", \"seg_combo_str\":\"<b>Segmentation Masks Present</b>\"})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:10.395528Z","iopub.execute_input":"2022-08-17T07:03:10.395762Z","iopub.status.idle":"2022-08-17T07:03:12.621823Z","shell.execute_reply.started":"2022-08-17T07:03:10.395730Z","shell.execute_reply":"2022-08-17T07:03:12.620526Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.2 INVESTIGATE THE IMAGE SIZES</h3>\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image slice sizes.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* Remember, there are **38,496** total examples.\n* Globally, we can see that 3 of the image shapes are **square** while one is **rectangular** and they all fall within a fairly tight distribution of relatively small sizes\n* Of these there are **4** unique sizes:\n    * $234 \\times 234$\n        * **Least frequent** image size\n        * **Smallest** image size\n        * Only **144** of the 38,496 occurences are this size (0.37%)\n    * $266 \\times 266$\n        * **Most frequent** image size\n        * **Second smallest** image size\n        * **25,920** of the 38,496 occurences are this size (67.33%)\n    * $276 \\times 276$\n        * **Second least frequent** image size\n        * **Second largest** image size\n        * **1,200** of the 38,496 occurences are this size (3.12%)\n    * $310 \\times 360$\n        * **Second most frequent** image size\n        * **Largest** image size\n        * **11,232** of the 38,496 occurences are this size (29.17%)\n\n\n\n","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"]), x=\"slice_w\", y=\"slice_h\", \n                 size=train_df.groupby([\"slice_w\", \"slice_h\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"]).index], \n                 color=\"(\"+train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])[\"slice_w\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])[\"slice_h\"].astype(str)+\")\", \n                 title=\"<b>Bubble Chart Showing The Various Image Sizes</b>\",\n                 labels={\"color\":\"<b>Size Legend</b>\", \n                         \"size\":\"<b>Number Of Observations</b>\",\n                         \"slice_h\":\"<b>Image Slice Height (pixels)</b>\",\n                         \"slice_w\":\"<b>Image Slice Width (pixels)</b>\"},\n                 size_max=160)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:12.623027Z","iopub.execute_input":"2022-08-17T07:03:12.623333Z","iopub.status.idle":"2022-08-17T07:03:12.767451Z","shell.execute_reply.started":"2022-08-17T07:03:12.623302Z","shell.execute_reply":"2022-08-17T07:03:12.766568Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.3 INVESTIGATE THE PIXEL SPACING</h3>\n\n---\n\nIt's observable that not all images have the same pixel spacing... however, given that, there is not that much variation between pixel spacing.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* Remember, there are **38,496** total examples.\n* Globally, we can see that all of the pixel spacings are **square** and that the vast majority are $1.50mm \\times 1.50mm$\n* There are only **2** unique sets of pixel spacings:\n    * $1.50mm \\times 1.50mm$\n        * **Most frequent** pixel spacing\n        * **Smallest** pixel spacing (barely)\n        * **37,296** of the 38,496 occurences are this size (96.88%)\n    * $1.63mm \\times 1.63mm$\n        * **Least frequent** image size\n        * **Largest** pixel spacing (barely)\n        * **1,200** of the 38,496 occurences are this size (3.12%)","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"]), x=\"px_spacing_w\", y=\"px_spacing_h\", \n                 size=train_df.groupby([\"px_spacing_w\", \"px_spacing_h\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"]).index], \n                 color=\"(\"+train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])[\"px_spacing_w\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])[\"px_spacing_h\"].astype(str)+\")\", \n                 title=\"<b>Bubble Chart Showing The Various Pixel Spacings</b>\",\n                 labels={\"color\":\"<b>Pixel Spacing Sets Legend</b>\", \n                         \"size\":\"<b>Number Of Observations</b>\",\n                         \"px_spacing_h\":\"<b>Pixel Spacing Height (mm)</b>\",\n                         \"px_spacing_w\":\"<b>Pixel Spacing Width (mm)</b>\"},\n                 size_max=160)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:12.768660Z","iopub.execute_input":"2022-08-17T07:03:12.768897Z","iopub.status.idle":"2022-08-17T07:03:12.857013Z","shell.execute_reply.started":"2022-08-17T07:03:12.768866Z","shell.execute_reply":"2022-08-17T07:03:12.856345Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.4 INVESTIGATE CASE IDS</h3>\n\n---\n\nHere's the host description of **`case_id`**\n\n> \"Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.\"\n\nI don't really observe any oddities associated with any particular **`case_id`** values. I would probably attempt to group them when stratifying/creating-folds... however, they don't seem to perpetrate an obvious bias.\n\nWhen we colour by **day**, we can see that all cases are made up (mostly) of groups of **144**, or less frequently, **80**, images from different days.","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df, train_df.case_id.astype(str), color=\"day_num_str\", title=\"<b>Distribution Of Images Per Case ID</b>\", \n             labels={\"x\":\"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"}, text_auto=True, width=2000)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:12.858234Z","iopub.execute_input":"2022-08-17T07:03:12.858799Z","iopub.status.idle":"2022-08-17T07:03:13.608293Z","shell.execute_reply.started":"2022-08-17T07:03:12.858756Z","shell.execute_reply":"2022-08-17T07:03:13.607368Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from matplotlib import gridspec\ndef plot_case(case_id, day=None, df=train_df, _figsize=(20,30), n_cols=16):\n    # Initialize\n    case_df = df[df.case_id==case_id]\n    \n    if day is not None:\n        _case_df = case_df[(case_df.day_num==day) | (case_df.day_num_str==str(day))]\n        if len(_case_df)>0:\n            approx_shrink = len(_case_df)/len(case_df)\n            case_df=_case_df\n            _figsize = (_figsize[0], int(np.ceil(1.25*_figsize[1]*approx_shrink)))\n        else:\n            print(\"There are no valid samples for the passed `day`. Reverting to all days in case.\")\n        del _case_df\n    \n    n_ex = len(case_df)\n    \n    print(\"...Preparing...\")\n    # Get relevant data\n    case_paths = case_df[\"f_path\"].tolist()\n    case_rles = [[_rle if not pd.isna(_rle) else None for _rle in _rles] for _rles in case_df[[\"lb_seg_rle\", \"sb_seg_rle\", \"st_seg_rle\"]].values.tolist()]\n    case_img_shapes = [(_w,_h) for _w,_h in zip(case_df[\"slice_w\"].tolist(), case_df[\"slice_h\"].tolist())]\n    all_overlays = [get_overlay(img_path, rle_strs, img_shape) for img_path, rle_strs, img_shape in zip(case_paths, case_rles, case_img_shapes)]\n    \n    print(\"...Plotting...\")    \n    # Plot\n    plt.figure(figsize=_figsize)\n    n_rows = int(np.ceil(n_ex/n_cols))\n    \n    gs = gridspec.GridSpec(n_rows, n_cols,\n         wspace=0.0, hspace=0.0, \n         top=1.-0.5/(n_rows+1), bottom=0.5/(n_rows+1), \n         left=0.5/(n_cols+1), right=1-0.5/(n_cols+1))\n    \n    for i in range(n_rows):\n        if len(all_overlays)==0: break\n        for j in range(n_cols):\n            if len(all_overlays)==0: break\n            ax=plt.subplot(gs[i,j])\n            ax.imshow(all_overlays.pop())\n            ax.axis(False)\n            ax.set_xticklabels([])\n            ax.set_yticklabels([])\n        \n    print(\"...Displaying...\")    \n    plt.show()\n        \nDEMO_CASE = 134\nprint(f\"\\n\\n... PLOTTING DEMO CASE ID #{DEMO_CASE} ...\\n\\n\")\nplot_case(DEMO_CASE)\n\nDEMO_CASE = 9\nprint(f\"\\n\\n\\n\\n... PLOTTING DEMO CASE ID #{DEMO_CASE} ...\\n\\n\")\nplot_case(DEMO_CASE)\n\nDEMO_CASE = 7\nDEMO_DAYS = [0, 13, 19]\nfor _dday in DEMO_DAYS:\n    print(f\"\\n\\n\\n... PLOTTING DEMO CASE ID #{DEMO_CASE} - FOR DAY #{_dday} ...\\n\\n\")\n    plot_case(DEMO_CASE, day=_dday)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:03:13.609714Z","iopub.execute_input":"2022-08-17T07:03:13.609956Z","iopub.status.idle":"2022-08-17T07:04:53.933840Z","shell.execute_reply.started":"2022-08-17T07:03:13.609925Z","shell.execute_reply":"2022-08-17T07:04:53.932855Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.5 MASK SIZES/AREAS</h3>\n\n---\n\nWe know that every other number in an RLE encoding represents a run of mask... so if we add up all those numbers we get the total number of masked pixels in an image. This is much faster than opening and closing each image.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* It's observable that the distributions of mask area is mostly normal although it skews slightly to the smaller side...\n* All the distributions are similar although the Stomach distribution has an odd gap between 400-750 pixels.\n* It's interesting to note that, while not common, we do have some VERY large masks (>7500 pixels)\n    * Also, it's kind of funny that the biggest masks are for **small** bowel\n    ","metadata":{}},{"cell_type":"code","source":"def get_mask_area(rle):\n    return sum([int(x) for x in rle.split()[1::2]])\n\ntrain_df[\"lb_seg_area\"] = train_df.lb_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\ntrain_df[\"sb_seg_area\"] = train_df.sb_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\ntrain_df[\"st_seg_area\"] = train_df.st_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\n\nfig = px.histogram(train_df, [\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"], title=\"<b>Mask Areas Overlaid</b>\", barmode=\"overlay\",\n                  labels={\"value\":\"<b>Mask Area</b>\"})\nfig.show()\n\nprint(\"\\n\\n\\n... EXAMINE AN EXAMPLE WITH A LARGE AMOUNT OF SEGMENTATION MASK ...\\n\")\nexamine_id(\"case134_day22_slice_0102\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:04:53.935093Z","iopub.execute_input":"2022-08-17T07:04:53.935465Z","iopub.status.idle":"2022-08-17T07:04:56.173869Z","shell.execute_reply.started":"2022-08-17T07:04:53.935431Z","shell.execute_reply":"2022-08-17T07:04:56.173021Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.6 MASK DATASET CREATION, CLASS OVERLAP & MASK HEATMAP</h3>\n\n---\n\nIt's important to determine if the the masks overlap one another (**multilabel**) or not (**multiclass**). To do this, we will quickly create a dataset of **`npy`** files. During this creation process we will check for overlap.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* There is overlap, and while it is not that common, some images exhibit a high degree of overlap.\n* This means that we cannot frame the problem as simple categorical semantic segmentation.\n* We must instead frame the problem as multi-label semantic segmentation\n* This means our mask will take the form --> $W \\times H \\times 3$\n    * Where the channel dimensions are binary masks for each respective segmentation type\n    * This will allow for the masks to overlap\n\n\n**NOTE ON THE PLOTTED IMAGE BELOW:**\n* In the examined image below we can see a section of the small bowel is completely inside of a larger section of larger bowel.\n* This shows why treating this as multi-label semantic segmentation is so important!","metadata":{}},{"cell_type":"code","source":"def is_overlap(_arr):\n    return _arr.sum(axis=-1).max()>1\n    \ndef make_seg_mask(row, output_dir=\"/kaggle/working/npy_files\", check_overlap=False):\n    slice_shape = (row.slice_w, row.slice_h)\n    if not pd.isna(row.lb_seg_rle):\n        lb_mask = rle_decode(row.lb_seg_rle, slice_shape, )\n    else:\n        lb_mask = np.zeros(slice_shape)\n    if not pd.isna(row.sb_seg_rle):\n        sb_mask = rle_decode(row.sb_seg_rle, slice_shape)\n    else:\n        sb_mask = np.zeros(slice_shape)\n    if not pd.isna(row.st_seg_rle):\n        st_mask = rle_decode(row.st_seg_rle, slice_shape)\n    else:\n        st_mask = np.zeros(slice_shape)\n    mask_arr = np.stack([lb_mask, sb_mask, st_mask], axis=-1).astype(np.uint8)\n    np.save(f\"./npy_files/{row.id}_mask\", mask_arr)\n    \n    if check_overlap: \n        if is_overlap(mask_arr): \n            return np.where(mask_arr.sum(axis=-1)>1, 1, 0).sum()\n        else:\n            return 0\n    \nNPY_DIR = \"/kaggle/working/npy_files\"\nif not os.path.isdir(NPY_DIR): os.makedirs(NPY_DIR, exist_ok=True)\ntrain_df[\"seg_overlap_area\"] = train_df.progress_apply(lambda x: make_seg_mask(x, output_dir=NPY_DIR, check_overlap=True), axis=1)\n\nprint(\"\\n... LET'S EXAMINE THE IMAGE WITH THE HIGHEST AMOUNT OF OVERLAP ...\\n\")\n\nexamine_id(train_df[train_df.seg_overlap_area==train_df.seg_overlap_area.max()].id.values[0])\n\nfig = px.histogram(train_df[train_df.seg_overlap_area>0], \"seg_overlap_area\", color=\"seg_combo_str\", nbins=50,\n                   log_y=True, title=\"<b>Distribution of Non-Zero Segmentation Overlaps <sub>(Count Is Logarithmic)</sub></b>\",  \n                   labels={\"seg_overlap_area\":\"<b>Area of Mask Overlap</b>\", \n                           \"seg_combo_str\":\"<b>Segmentation Masks In Image</b>\"})\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"right\",\n    x=0.995\n))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:04:56.175314Z","iopub.execute_input":"2022-08-17T07:04:56.176201Z","iopub.status.idle":"2022-08-17T07:07:47.313690Z","shell.execute_reply.started":"2022-08-17T07:04:56.176137Z","shell.execute_reply":"2022-08-17T07:07:47.312678Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"heatmap = np.zeros((256,256,3), dtype=np.float32)\nfor _, _row in tqdm(train_df.iterrows(), total=len(train_df)):\n    if (_row.lb_seg_flag or _row.sb_seg_flag or _row.st_seg_flag):\n        _mask = cv2.resize(np.load(f\"./npy_files/{_row.id}_mask.npy\"), (256,256), interpolation=cv2.INTER_NEAREST)\n        heatmap+=_mask\n\nheatmap=heatmap/heatmap.max()\n\nplt.figure(figsize=(20,12))\n\nplt.subplot(1,4,1)\nplt.imshow(heatmap[..., 0], cmap=\"magma\")\nplt.title(\"Large Bowel Segmentation Mask ‚Äì Heat Map\", fontweight=\"bold\")\nplt.axis(False)\n\nplt.subplot(1,4,2)\nplt.imshow(heatmap[..., 1], cmap=\"magma\")\nplt.title(\"Small Bowel Segmentation Mask ‚Äì Heat Map\", fontweight=\"bold\")\nplt.axis(False)\n\nplt.subplot(1,4,3)\nplt.imshow(heatmap[..., 2], cmap=\"magma\")\nplt.title(\"Stomach Segmentation Mask ‚Äì Heat Map\", fontweight=\"bold\")\nplt.axis(False)\n\nplt.subplot(1,4,4)\nplt.imshow(heatmap)\nplt.title(\"All Sementation Masks Combined ‚Äì Heat Map\", fontweight=\"bold\")\nplt.axis(False)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:07:47.315198Z","iopub.execute_input":"2022-08-17T07:07:47.315529Z","iopub.status.idle":"2022-08-17T07:08:22.118981Z","shell.execute_reply.started":"2022-08-17T07:07:47.315486Z","shell.execute_reply":"2022-08-17T07:08:22.118095Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.7 PIXEL VALUES IN OUR DATASET</h3>\n\n---\n\nIt's important to analyse the dataset because we will need to normalize the data to convert it into a format that is more expected for machine learning (uint8 (0-255) or float32 (0-1)). Without knowing the limits of the images, we may diminish the resolution of the data by accident when normalizing.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\nInterestingly the maximum value in the dataset is equiavlent to less than half of an int16 or a quarter of a uint16.\n* Max Value for UINT16\n    * **65535**\n* Max Value for INT16\n    * **32767**\n* Half of Max Value for INT16\n    * **16384**\n* Actual Max Value in the dataset\n    * **15865**","metadata":{}},{"cell_type":"code","source":"def get_image_vals(row):\n    _img = cv2.imread(row.f_path, -1)\n    _nonzero_px_count = np.count_nonzero(_img)\n    \n    row[\"nonzero_num_pxs\"] = _nonzero_px_count\n    row[\"max_px_value\"] = _img.max()\n    row[\"min_px_value\"] = _img.min()\n    row[\"mean_px_value\"] = _img.mean()\n    row[\"nonzero_mean_px_value\"] = _img.sum()/_nonzero_px_count\n    \n    return row\n\ntrain_df = train_df.progress_apply(get_image_vals, axis=1)\n\nprint(f\"\\n\\n\\n... UPDATED TRAIN DATAFRAME ...\\n\")\ndisplay(train_df.head())\nprint(\"\\n\\n\")\n\nfor _c in [\"nonzero_num_pxs\", \"max_px_value\", \"min_px_value\", \"mean_px_value\", \"nonzero_mean_px_value\"]:\n    print(f\"\\n... STATS FOR COLUMN --> `{_c}`...\")\n    print(f\"\\t--> MIN  VAL: {train_df[_c].min():.1f}\")\n    print(f\"\\t--> MEAN VAL: {train_df[_c].mean():.1f}\")\n    print(f\"\\t--> MAX  VAL: {train_df[_c].max():.1f}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:08:22.120361Z","iopub.execute_input":"2022-08-17T07:08:22.120951Z","iopub.status.idle":"2022-08-17T07:18:43.453809Z","shell.execute_reply.started":"2022-08-17T07:08:22.120905Z","shell.execute_reply":"2022-08-17T07:18:43.452803Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.8 IDENTIFY ANY HEURISTICS OR RULES REGARDING SEGMENTATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SLICEWISE OBSERVATIONS</b>\n\nFor a given **case-id** and **day number** there are two different amounts of scans present\n* 144 slices --> 259 instances\n* 80 slices ---> 15 instances\n\n* There are no examples for slices number **1, 138, 139, 140, 141, 142, 143 or 144** that have any segmentation masks\n* If we break it down by organ we get the following no-value slices for each respective organ\n    * **Large Bowel** ‚Äì **1, 138, 139, 140, 141, 142, 143, 144**\n    * **Small Bowel** ‚Äì **1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 138, 139, 140, 141, 142, 143, 144**\n    * **Stomach** ‚Äì **1, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144**\n    \n<br><br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">ROLLING SLICE OBSERVATIONS</b>\n\nWe next want to identify if there are ever any occurences of a segmentation mask present when there are no segmentation masks before or after (ideally we can measure the prevelance if this exists, and if it doesn't exist, we can identify the minimum number of contiguous segments required).","metadata":{}},{"cell_type":"code","source":"train_df[\"lb_seg_isolated\"] = False\ntrain_df[\"sb_seg_isolated\"] = False\ntrain_df[\"st_seg_isolated\"] = False\n\ntrain_df.loc[\n    (train_df[\"lb_seg_flag\"]==True) &\n    (train_df[\"lb_seg_flag\"]!=train_df[\"lb_seg_flag\"].shift(1, fill_value=False)) &\n    (train_df[\"lb_seg_flag\"]!=train_df[\"lb_seg_flag\"].shift(-1, fill_value=False)), \"lb_seg_isolated\"\n] = True\n\ntrain_df.loc[\n    (train_df[\"sb_seg_flag\"]==True) &\n    (train_df[\"sb_seg_flag\"]!=train_df[\"sb_seg_flag\"].shift(1, fill_value=False)) &\n    (train_df[\"sb_seg_flag\"]!=train_df[\"sb_seg_flag\"].shift(-1, fill_value=False)), \"sb_seg_isolated\"\n] = True\n\ntrain_df.loc[\n    (train_df[\"st_seg_flag\"]==True) &\n    (train_df[\"st_seg_flag\"]!=train_df[\"st_seg_flag\"].shift(1, fill_value=False)) &\n    (train_df[\"st_seg_flag\"]!=train_df[\"st_seg_flag\"].shift(-1, fill_value=False)), \"st_seg_isolated\"\n] = True\n\n# case43_day18 (lb), case138_day0 (lb), case7_day0 (st)\ntrain_df[train_df.lb_seg_isolated]","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:18:43.455236Z","iopub.execute_input":"2022-08-17T07:18:43.455495Z","iopub.status.idle":"2022-08-17T07:18:43.542881Z","shell.execute_reply.started":"2022-08-17T07:18:43.455464Z","shell.execute_reply":"2022-08-17T07:18:43.542082Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"examine_id(\"case7_day0_slice_0052\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:18:43.544214Z","iopub.execute_input":"2022-08-17T07:18:43.544803Z","iopub.status.idle":"2022-08-17T07:18:43.921202Z","shell.execute_reply.started":"2022-08-17T07:18:43.544768Z","shell.execute_reply":"2022-08-17T07:18:43.920252Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_df[\"slice_count\"] = train_df.id.apply(lambda x: int(x.rsplit(\"_\", 1)[-1]))\n\nprint(\"\\n... CASE-ID/DAY-NUM SLICE INFORMATION ...\\n\")\ntrain_df.groupby([\"case_id\", \"day_num\"])[\"slice_count\"].max().value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:18:43.922832Z","iopub.execute_input":"2022-08-17T07:18:43.923351Z","iopub.status.idle":"2022-08-17T07:18:43.975467Z","shell.execute_reply.started":"2022-08-17T07:18:43.923315Z","shell.execute_reply":"2022-08-17T07:18:43.974586Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"slice_to_occurence_df = train_df.groupby(\"slice_count\")[[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"]].sum().reset_index()\nfig = px.bar(slice_to_occurence_df, \n             x=\"slice_count\", y=[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"],\n             orientation=\"v\", labels={\n                 \"slice_count\":\"<b>Slice Number</b>\", \n                 \"value\":\"<b>Number Of Examples</b>\",\n             }, title=\"<b>Number of Examples Per Example For Our 3 Organs</b>\")\n\nfig.update_layout(\n    legend_title=\"<b>Organ Type Legend</b>\"\n)\n    \nfig.show()\n\nprint(\"\\n... WHICH SLICES ARE ALWAYS BLANK (NO SEG) BY LABEL ...\\n\")\nkeep_slice_blank_map = {_sh_lbl:slice_to_occurence_df[slice_to_occurence_df[f\"{_sh_lbl}_seg_flag\"]==0].slice_count.to_list() for _sh_lbl in [\"lb\", \"sb\", \"st\"]}\nkeep_slice_blank_map","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:18:43.977093Z","iopub.execute_input":"2022-08-17T07:18:43.978032Z","iopub.status.idle":"2022-08-17T07:18:44.090946Z","shell.execute_reply.started":"2022-08-17T07:18:43.977983Z","shell.execute_reply":"2022-08-17T07:18:44.090179Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.9 CREATE 3D GIF FOR CASE>DAY GROUPS OF SLICES (WITH MASK!!)</h3>\n\n---\n\n**NOTE: We delete the npy files here because we need to save soon and those will muck things up**\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\nIt's beautiful!","metadata":{}},{"cell_type":"code","source":"!rm -rf ./npy_files","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-17T07:18:44.092052Z","iopub.execute_input":"2022-08-17T07:18:44.092305Z","iopub.status.idle":"2022-08-17T07:18:47.289174Z","shell.execute_reply.started":"2022-08-17T07:18:44.092275Z","shell.execute_reply":"2022-08-17T07:18:47.287882Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    _img = open_gray16(img_path, to_rgb=True)\n    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) if (rle_str is not None and not pd.isna(rle_str)) else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay\n\ndef create_animation(case_id, day_num, df=train_df, save=False, save_dir=\"/kaggle/working/animations\"):\n    \n    sub_df = df[(df.case_id==case_id) & (df.day_num==day_num)]\n    \n    f_paths  = sub_df.f_path.tolist()\n    lb_rles  = sub_df.lb_seg_rle.tolist()\n    sb_rles  = sub_df.sb_seg_rle.tolist()\n    st_rles  = sub_df.st_seg_rle.tolist()\n    slice_ws = sub_df.slice_w.tolist()\n    slice_hs = sub_df.slice_h.tolist()\n    \n    animation_arr = np.stack([\n        get_overlay(img_path=_f, rle_strs=(_lb, _sb, _st), img_shape=(_w, _h)) \\\n        for _f, _lb, _sb, _st, _w, _h in \\\n        zip(f_paths, lb_rles, sb_rles, st_rles, slice_ws, slice_hs)\n    ], axis=0)\n    \n    fig = plt.figure(figsize=(8,8))\n    \n    plt.axis('off')\n    im = plt.imshow(animation_arr[0])\n    plt.title(f\"3D Animation for Case {case_id} on Day {day_num}\", fontweight=\"bold\")\n    \n    def animate_func(i):\n        im.set_array(animation_arr[i])\n        return [im]\n    plt.close()\n    \n    anim = animation.FuncAnimation(fig, animate_func, frames = animation_arr.shape[0], interval = 1000//12)\n    if save:\n        if not os.path.isdir(save_dir): os.makedirs(save_dir, exist_ok=True)\n        anim.save(os.path.join(save_dir, f\"case_{case_id}_day_{day_num}.gif\"), fps=10, writer='imagemagick')\n    return anim\n\ncreate_animation(case_id=115, day_num=0, save=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:18:47.291554Z","iopub.execute_input":"2022-08-17T07:18:47.291932Z","iopub.status.idle":"2022-08-17T07:19:55.621973Z","shell.execute_reply.started":"2022-08-17T07:18:47.291882Z","shell.execute_reply":"2022-08-17T07:19:55.621166Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**As an aside let's create gifs for the 3 case>day pairings that we discovered above have non-contiguous masks**\n* Some of them definitely have some weird artefacts like the blinking annotation in the last animation","metadata":{}},{"cell_type":"code","source":"# case43_day18 (lb)\nprint(\"\\n\\n\\n... LARGE BOWEL ‚Äì NON-CONTIGUOUS EXAMPLE 1 ...\\n\\n\")\ncreate_animation(case_id=43, day_num=18, save=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:19:55.626781Z","iopub.execute_input":"2022-08-17T07:19:55.627331Z","iopub.status.idle":"2022-08-17T07:21:00.589744Z","shell.execute_reply.started":"2022-08-17T07:19:55.627260Z","shell.execute_reply":"2022-08-17T07:21:00.587874Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# case138_day0 (lb)\nprint(\"\\n\\n\\n... LARGE BOWEL ‚Äì NON-CONTIGUOUS EXAMPLE 2 ...\\n\\n\")\ncreate_animation(case_id=138, day_num=0, save=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:21:00.597291Z","iopub.execute_input":"2022-08-17T07:21:00.597630Z","iopub.status.idle":"2022-08-17T07:22:06.374389Z","shell.execute_reply.started":"2022-08-17T07:21:00.597585Z","shell.execute_reply":"2022-08-17T07:22:06.371661Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# case7_day0 (st)\nprint(\"\\n\\n\\n... STOMACH ‚Äì NON-CONTIGUOUS EXAMPLE 1 ...\\n\")\ncreate_animation(case_id=7, day_num=0, save=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:22:06.376908Z","iopub.execute_input":"2022-08-17T07:22:06.377470Z","iopub.status.idle":"2022-08-17T07:23:13.043227Z","shell.execute_reply.started":"2022-08-17T07:22:06.377381Z","shell.execute_reply":"2022-08-17T07:23:13.042326Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.10 SCANS WITH ERRORS</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n[**Paul G**](https://www.kaggle.com/pgeiger) identified two cases with errors in the segmentation masks in [**this discussion post**](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319963)\n\n* Case 7 ‚Äì Day 0\n\n<img src=\"https://i.ibb.co/M8p8Xfk/case7-day0-slice-0096.png\">\n\n<br>\n\n* Case 81 ‚Äì Day 30\n\n<img src=\"https://i.ibb.co/jkdcdzR/case81-day30-slice-0096.png\">\n\n---\n\nIn addition to this, we can see from the above animations that other case/day pairings have errors (or at the very least strong inconsistencies). We plot the remaining error scan below.","metadata":{}},{"cell_type":"code","source":"problem_case_1 = 7\nproblem_day_1  = 0\nproblem_case_2 = 81\nproblem_day_2  = 30\n\nprint(\"\\n... PROBLEM CASE NUMBER 1 ...\\n\")\nplot_case(problem_case_1, day=problem_day_1)\n\nprint(\"\\n... PROBLEM CASE NUMBER 2 ...\\n\")\nplot_case(problem_case_2, day=problem_day_2)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:23:13.044721Z","iopub.execute_input":"2022-08-17T07:23:13.045112Z","iopub.status.idle":"2022-08-17T07:23:30.423133Z","shell.execute_reply.started":"2022-08-17T07:23:13.045078Z","shell.execute_reply":"2022-08-17T07:23:30.422257Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\n\\n... PLOT REMAINING ERROR SCAN ...\\n\")\ncreate_animation(case_id=81, day_num=30, save=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:23:30.424499Z","iopub.execute_input":"2022-08-17T07:23:30.424734Z","iopub.status.idle":"2022-08-17T07:24:36.111247Z","shell.execute_reply.started":"2022-08-17T07:23:30.424705Z","shell.execute_reply":"2022-08-17T07:24:36.110332Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.11 APPROXIMATE CLASS WEIGHTING (CATEGORICAL ASSUMPTION)</h3>\n\n---\n\nLet us calculate a naive approximation of the weights of classes based on the frequency of occurence of various classes. For the purpose of this investigation we will treat the background as it's own class (the most common class probably).\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* Class **0** - **Background**\n  * Total Pixel Count In Training Dataset = **0.00**\n  * Percentage Of All Pixels In Training Dataset = **0.00**\n  \n<br>\n\n* Class **1** - **Large Bowel**\n  * Total Pixel Count In Training Dataset = **0.00**\n  * Percentage Of All Pixels In Training Dataset = **0.00**\n  \n<br>\n  \n* Class **2** - **Small Bowel**\n  * Total Pixel Count In Training Dataset = **0.00**\n  * Percentage Of All Pixels In Training Dataset = **0.00**\n \n<br>\n  \n* Class **3** - **Stomach**\n  * Total Pixel Count In Training Dataset = **0.00**\n  * Percentage Of All Pixels In Training Dataset = **0.00**\n  \n<br>","metadata":{}},{"cell_type":"code","source":"# # Get total image area\ntrain_df[\"img_px_area\"] = train_df[\"slice_w\"]*train_df[\"slice_h\"]\ntrain_df[[\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"]].fillna(0, inplace=True)\ntrain_df[\"bg_area\"] = (train_df[\"img_px_area\"] - train_df[[\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"]].sum(axis=1)).astype(int)\n\nprint(f\"\\nALL TRAINING DATA PIXEL COUNT         : {train_df.img_px_area.sum()}\")\nprint(f\"BACKGROUND TRAINING DATA PIXEL COUNT  : {train_df.bg_area.sum()}\")\nprint(f\"LARGE BOWEL TRAINING DATA PIXEL COUNT : {train_df.lb_seg_area.sum()}\")\nprint(f\"SMALL BOWEL TRAINING DATA PIXEL COUNT : {train_df.sb_seg_area.sum()}\")\nprint(f\"STOMACH TRAINING DATA PIXEL COUNT     : {train_df.st_seg_area.sum()}\\n\")\n\nprint(f\"\\nALL TRAINING DATA PIXEL COUNT (%)         : %{100:.4f}\")\nprint(f\"BACKGROUND TRAINING DATA PIXEL COUNT (%)  : %{100*train_df.bg_area.sum()/train_df.img_px_area.sum():.4f}\")\nprint(f\"LARGE BOWEL TRAINING DATA PIXEL COUNT (%) : %{100*train_df.lb_seg_area.sum()/train_df.img_px_area.sum():.4f}\")\nprint(f\"SMALL BOWEL TRAINING DATA PIXEL COUNT (%) : %{100*train_df.sb_seg_area.sum()/train_df.img_px_area.sum():.4f}\")\nprint(f\"STOMACH TRAINING DATA PIXEL COUNT (%)     : %{100*train_df.st_seg_area.sum()/train_df.img_px_area.sum():.4f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:24:36.112791Z","iopub.execute_input":"2022-08-17T07:24:36.113536Z","iopub.status.idle":"2022-08-17T07:24:36.145946Z","shell.execute_reply.started":"2022-08-17T07:24:36.113497Z","shell.execute_reply":"2022-08-17T07:24:36.144972Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"modelling\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: teal;\" id=\"modelling\">5&nbsp;&nbsp;MODELLING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">5.1 TEST FORMATTING SUBMISSION</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"slice_px_map = {}\nfor _, row in train_df.groupby([\"slice_h\", \"slice_w\", \"px_spacing_h\", \"px_spacing_w\", \"slice_id\"])[[\"lb_seg_rle\", \"sb_seg_rle\", \"st_seg_rle\"]].first().reset_index().iterrows():\n    slice_px_map[f\"{row['slice_h']}-{row['slice_w']}-{row['px_spacing_h']}-{row['px_spacing_w']}-{row['slice_id']}-large_bowel\"] = row[\"lb_seg_rle\"]\n    slice_px_map[f\"{row['slice_h']}-{row['slice_w']}-{row['px_spacing_h']}-{row['px_spacing_w']}-{row['slice_id']}-small_bowel\"] = row[\"sb_seg_rle\"]\n    slice_px_map[f\"{row['slice_h']}-{row['slice_w']}-{row['px_spacing_h']}-{row['px_spacing_w']}-{row['slice_id']}-stomach\"] = row[\"st_seg_rle\"]\n\nss_df[\"ident\"] = ss_df['slice_h'].astype(str)+\"-\"+ss_df['slice_w'].astype(str)+\"-\"+ss_df['px_spacing_h'].astype(str)+\"-\"+ss_df['px_spacing_w'].astype(str)+\"-\"+ss_df['slice_id'].astype(str)+\"-\"+ss_df[\"class\"].astype(str)\nss_df[\"predicted\"] = ss_df[\"ident\"].map(slice_px_map)\nss_df = ss_df[[\"id\", \"class\", \"predicted\"]]\n\nss_df.to_csv(\"submission.csv\", index=False)\ndisplay(ss_df)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:24:36.147267Z","iopub.execute_input":"2022-08-17T07:24:36.147510Z","iopub.status.idle":"2022-08-17T07:24:36.284704Z","shell.execute_reply.started":"2022-08-17T07:24:36.147480Z","shell.execute_reply":"2022-08-17T07:24:36.283532Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">5.2 DICE SCORE EXPLAINED USING SOME OF OUR PREDICTED MASKS</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">F1 SCORE</b>\n\n<sub><b>REFERENCE --> This <a href=\"https://towardsdatascience.com/essential-things-you-need-to-know-about-f1-score-dbd973bf1a3\">Brilliant Medium Article</a> --> Please Give it Some Claps!</b></sub>\n\n<br>\n\nBy definition, F1-score is the harmonic mean of precision and recall. It combines precision and recall into a single number using the following formula:\n\n<center><img src=\"https://miro.medium.com/max/530/0*1dd6yQxvpZxpdjBX\"></center>\n\n<br>\n\nThis formula can also be equivalently written as,\n\n<center><img src=\"https://miro.medium.com/max/426/0*JFYDif-juwSY5TFI\"></center>\n\n<br>\n\nRemember the definition of Precision and Recall are...\n\n<center><img src=\"https://miro.medium.com/max/888/1*7J08ekAwupLBegeUI8muHA.png\"></center>\n\n<br>\n\nNotice that F1-score takes both precision and recall into account, which also means it accounts for both FPs and FNs. The higher the precision and recall, the higher the F1-score. F1-score ranges between 0 and 1. The closer it is to 1, the better the model.\n\n---\n\nNow that we‚Äôve covered the fundamentals, let‚Äôs walk through the thinking process of choosing between precision, recall and F1-score. Suppose we have trained three different models for cancer prediction, and each model has different precision and recall values.\n\n<center><img src=\"https://miro.medium.com/max/1400/1*cLzC3-sdOZ5hLc5Y68dEog.gif\"></center>\n\n* If we assess that errors caused by FPs (Scenario #2 in Figure 1) are more undesirable, then we will select a model based on precision and choose Model C.\n* If we assess that errors caused by FNs (Scenario #3 in Figure 1) are more undesirable, then we will select a model based on recall and choose Model B.\n* However, if we assess that both types of errors are undesirable, then we will select a model based on F1-score and choose Model A.\n\nSo, the takeaway here is that the model you select depends greatly on the evaluation metric you choose, which in turn depends on the relative impacts of errors of FPs and FNs in your use-case.\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">DICE SCORE</b>\n\n<br>\n\nThe very simple explanation here... is <b>that the Dice Score is simply <font color=\"red\">2*F1-Score</font></b> The other metric we should consider understanding is the Jaccard (IoU) Score. For clarity both those formulas are shown below:\n\n<br>\n\n$$\nDice = \\frac{2 TP}{2TP+FP+FN}, \\qquad Jaccard = IoU =  \\frac{TP}{TP+FP+FN}\n$$\n\n<br>\n\n**And here is a visualization of the dice score**\n\n<center><img src=\"https://miro.medium.com/max/858/1*yUd5ckecHjWZf6hGrdlwzA.png\"></center>\n\n<br>\n\n---\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">3D Hausdorff Distance</b>\n\n<br>\n\n**WIKIPEDIA DEFINITION**\n\nIn mathematics, the Hausdorff distance, or Hausdorff metric, also called Pompeiu‚ÄìHausdorff distance,[1][2] measures how far two subsets of a metric space are from each other. It turns the set of non-empty compact subsets of a metric space into a metric space in its own right. It is named after Felix Hausdorff and Dimitrie Pompeiu.\n\nInformally, two sets are close in the Hausdorff distance if every point of either set is close to some point of the other set. The Hausdorff distance is the longest distance you can be forced to travel by an adversary who chooses a point in one of the two sets, from where you then must travel to the other set. In other words, it is the greatest of all the distances from a point in one set to the closest point in the other set.\n\nThis distance was first introduced by Hausdorff in his book Grundz√ºge der Mengenlehre, first published in 1914, although a very close relative appeared in the doctoral thesis of Maurice Fr√©chet in 1906, in his study of the space of all continuous curves from $[0,1]\\to \\mathbb {R} ^{3}$\n\n<br>\n\n**REDDIT ELI5 DEFINITION**\n\nLet's say I give you a bunch of objects. For each pair of those objects, I give you a number that we'll call the \"distance\" between them. It doesn't matter whether the number is really the distance between them‚Äîif you move the objects around the number stays the same‚Äîwe're just going to call it that.\n\nNow, let's say you pick some of those objects and split them into two groups. We can ask if there's a good way to define the \"distance\" between those two groups. The answer given by Hausdorff, called the Hausdorff distance, is as follows:\n\nFor each thing in the first set, find the thing in the second set it's closest to and write down that distance. Then, after you've done that for each thing in the first set, take the biggest number you just wrote down.\n\nNow switch. For each thing in the second set, find the thing it's closed to in the first set and write down that distance. Then, after you've done that for each thing in the second set, take the biggest number you just wrote down.\n\nNow you have two numbers. The bigger of those is the Hausdorff distance.\n\nNow, that's all well and good if there's only a finite number of objects involved. If there are an infinite number of objects, then you have to be a little more careful about \"closest\" and \"biggest\" in finding the two numbers above, but it's still basically the same idea.\n\nAn example might help. Let's take our objects to be the numbers {1, 2, 3, 4, 5, 6}. We'll call the distance between those numbers their difference, so the distance between, say, 2 and 5 is 3. Alright, now let's consider the Hausdorff distance between the groups {1, 2, 3} and {2, 6}.\n\nFirst, we take 1 and find that the smallest distance between it and anything in the second group is 1. Then we take 2 and find that the smallest distance is 0, then we take 3 and we find that the smallest distance is 1. The biggest of these distances is 1.\n\nNow we take 2 and find that the smallest distance to an element in the first set is 0, and we do so for 6 and get 3. The biggest of these distances is 3.\n\nTaking the bigger of our two numbers, we see that the Hausdorff distance between {1, 2, 3} and {2, 6} is 3.\n\n---\n\n#### <b><center><font color=\"red\">THIS IS WIP ‚Äì DISREGARD FOR NOW</font></center></b>\n\n---\n\n<br>","metadata":{}},{"cell_type":"code","source":"from scipy.spatial._hausdorff import directed_hausdorff\n\nf1score = tfa.metrics.F1Score(num_classes=3)\ndef tf_hausdorff_distance(point_set_a, point_set_b):\n    difference = point_set_a-point_set_b\n    # Calculate the square distances between each two points: |ai - bj|^2.\n    square_distances = tf.einsum(\"...i,...i->...\", difference, difference)\n    minimum_square_distance_a_to_b = tf.math.reduce_min(square_distances, axis=-1)\n    return tf.math.sqrt(tf.math.reduce_max(minimum_square_distance_a_to_b, axis=-1))\n\ndef HausdorffDist(A,B):\n    # Find pairwise distance\n    D_mat = np.sqrt(inner1d(A,A)[np.newaxis].T + inner1d(B,B)-2*(np.dot(A,B.T)))\n    # Find DH\n    dH = np.max(np.array([np.max(np.min(D_mat,axis=0)),np.max(np.min(D_mat,axis=1))]))\n    return(dH)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:24:36.286270Z","iopub.execute_input":"2022-08-17T07:24:36.286523Z","iopub.status.idle":"2022-08-17T07:24:36.429459Z","shell.execute_reply.started":"2022-08-17T07:24:36.286491Z","shell.execute_reply":"2022-08-17T07:24:36.428801Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"DEMO_CASE_1 = \"case123_day20\" # Assume this is the ground truth\nDEMO_CASE_2 = \"case123_day0\" # Assume this is the ground truth\n\nlb_seg_arr_1 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_1)].lb_seg_rle ], axis=0).astype(np.double)\nsb_seg_arr_1 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_1)].sb_seg_rle ], axis=0).astype(np.double)\nst_seg_arr_1 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_1)].st_seg_rle ], axis=0).astype(np.double)\n\nlb_seg_arr_2 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_2)].lb_seg_rle ], axis=0).astype(np.double)\nsb_seg_arr_2 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_2)].sb_seg_rle ], axis=0).astype(np.double)\nst_seg_arr_2 = np.stack([rle_decode(x, (266,266)) if not pd.isna(x) else np.zeros((266,266), dtype=np.uint8) for x in train_df[train_df.id.str.contains(DEMO_CASE_2)].st_seg_rle ], axis=0).astype(np.double)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T07:57:36.615486Z","iopub.execute_input":"2022-08-17T07:57:36.616224Z","iopub.status.idle":"2022-08-17T07:57:36.954584Z","shell.execute_reply.started":"2022-08-17T07:57:36.616172Z","shell.execute_reply":"2022-08-17T07:57:36.953605Z"},"trusted":true},"execution_count":40,"outputs":[]}]}